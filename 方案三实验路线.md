# 方案三 (DINOEncoderDecoder) 实验测试路线

## 方案概述

方案三采用 DINOv3 作为唯一编码器，完全替代 Restormer Encoder，配合专门设计的 FPN 风格解码器：

- 多尺度特征提取 (Layer 4/8/12)
- FPN 风格解码器 + PixelShuffle 上采样
- PatchBoundaryBlender 处理块效应
- 可选 LoRA 微调让 DINO 适应低光照特性

核心优势：最大化利用 DINO 预训练特征，参数效率高，适合追求最佳效果的场景。

---

## 阶段一：基线实验

```
实验 1.1: 冻结 DINO 基线
├── 配置: LowLight_DINOEncoderDecoder.yml
├── 设置: use_lora=false, gamma=0.35
├── 目标: 建立基线 PSNR/SSIM
└── 预计显存: ~6-8 GB

实验 1.2: Gamma 敏感性测试
├── 配置: 修改 dino_gamma
├── 测试值: [0.30, 0.35, 0.40, 0.45, 0.50]
├── 目标: 找到最优 gamma
└── 方法: 可以只跑 20k iter 快速对比
```

## 阶段二：LoRA 微调实验

```
实验 2.1: LoRA 基础实验
├── 配置: LowLight_DINOEncoderDecoder_LoRA.yml
├── 设置: use_lora=true, gamma=0.4, lora_r=16
├── 目标: 对比 LoRA 是否提升
└── 预计显存: ~12-14 GB

实验 2.2: LoRA 秩敏感性 (可选)
├── 测试值: lora_r = [8, 16, 32]
├── 目标: 找到参数量与效果的平衡
└── 注意: r 越大，参数越多，过拟合风险越高

实验 2.3: LoRA Alpha 调优 (可选)
├── 测试值: lora_alpha = [16, 32, 64] (固定 r=16)
├── 目标: 调整 LoRA 缩放因子
└── 经验: alpha = 2 × r 通常效果较好
```

## 阶段三：消融实验

```
实验 3.1: 块效应处理消融
├── 对比: use_boundary_blend = [true, false]
├── 目标: 验证 PatchBoundaryBlender 的作用
└── 观察: 输出图像是否有网格状伪影

实验 3.2: 多尺度特征消融
├── 对比: extract_layers = [[12], [8,12], [4,8,12]]
├── 目标: 验证浅层特征是否有帮助
└── 注意: 单层时需要修改 FPNDecoder

实验 3.3: 解码器通道数消融 (可选)
├── 对比: hidden_dims = [[256,128,64,32], [384,192,96,48], [512,256,128,64]]
├── 目标: 找到解码器容量与效果的平衡
└── 注意: 通道数越大，参数越多
```

## 阶段四：损失函数调优

```
实验 4.1: 语义损失配置
├── 测试值: semantic_distance = ['l1', 'l2', 'cosine']
├── 目标: 找到最优的语义距离度量
└── 推荐: cosine 通常效果较好

实验 4.2: 语义损失层选择
├── 测试值: semantic_layers = [[12], [8,12], [4,8,12]]
├── 目标: 验证不同层语义损失的贡献
└── 注意: 多层会增加计算量
```

---

## 推荐执行顺序

| 优先级 | 实验 | 配置文件 | 预计时间 |
|-------|------|---------|---------|
| 1 | 1.1 冻结基线 | `LowLight_DINOEncoderDecoder.yml` | 8-10h |
| 2 | 1.2 Gamma 测试 | 修改 gamma, 跑 20k iter | 2h × 5 |
| 3 | 2.1 LoRA 实验 | `LowLight_DINOEncoderDecoder_LoRA.yml` | 10-12h |
| 4 | 3.1 块效应消融 | 修改 use_boundary_blend | 8h |
| 5 | 2.2 LoRA 秩消融 | 修改 lora_r | 10h × 2 |
| 6 | 3.2 提取层消融 | 修改 extract_layers | 8h × 2 |

---

## 实验记录模板

| 实验 | Gamma | LoRA | PSNR | SSIM | LPIPS | 显存 | 备注 |
|-----|-------|------|------|------|-------|------|------|
| 1.1 | 0.35 | ❌ | | | | ~7GB | 基线 |
| 1.2a | 0.30 | ❌ | | | | | |
| 1.2b | 0.40 | ❌ | | | | | |
| 1.2c | 0.45 | ❌ | | | | | |
| 1.2d | 0.50 | ❌ | | | | | |
| 2.1 | 0.40 | ✅ r=16 | | | | ~13GB | |
| 2.2a | 0.40 | ✅ r=8 | | | | | |
| 2.2b | 0.40 | ✅ r=32 | | | | | |
| 3.1 | 最优 | 最优 | | | | | blend=false |
| 3.2a | 最优 | 最优 | | | | | layers=[12] |
| 3.2b | 最优 | 最优 | | | | | layers=[8,12] |

---

## 运行命令

```bash
# 实验 1.1: 冻结 DINO 基线
python basicsr/train.py -opt LLIE/Options/LowLight_DINOEncoderDecoder.yml

# 实验 2.1: LoRA 微调
python basicsr/train.py -opt LLIE/Options/LowLight_DINOEncoderDecoder_LoRA.yml
```

---

## 关键配置参数说明

```yaml
network_g:
  # DINOv3 编码器配置
  dino_model: dinov3_vithplus16
  dino_gamma: 0.35                    # 低光照预处理 gamma
  extract_layers: [4, 8, 12]          # 多尺度特征提取层
  
  # FPN 解码器配置
  hidden_dims: [384, 192, 96, 48]     # 解码器各层通道数
  use_boundary_blend: true            # 块效应处理
  
  # LoRA 微调配置
  use_lora: false                     # 是否启用 LoRA
  lora_r: 16                          # LoRA 秩
  lora_alpha: 32                      # LoRA 缩放因子
  lora_dropout: 0.1                   # LoRA dropout
```

---

## 配置文件列表

- `LLIE/Options/LowLight_DINOEncoderDecoder.yml` - 冻结 DINO 基线
- `LLIE/Options/LowLight_DINOEncoderDecoder_LoRA.yml` - LoRA 微调版本
