# 实验结果汇总

> 最后更新: 2026-01-14

## 实验总结表格

| 实验ID | 架构 | 融合方式 | Patch Size | 迭代次数 | 最佳PSNR | 最佳iter | 最终PSNR | 训练时长 | 状态 |
|--------|------|----------|------------|----------|----------|----------|----------|----------|------|
| 11 | DINOGuidedRestormer | SFT | 128 | 60k | 22.08 | 36k | 21.11 | 3h33m | 完成 |
| 12 | DINOGuidedRestormer | SFT | 192 | 300k | **22.87** | 90k | 20.60 | 19h37m | Early Stop |
| 13 | DINOGuidedRestormer | SFT+EMA | 192 | 80k | 21.94 | 74k | 21.88 | 10h58m | 完成 |
| 14 | DINOGuidedRestormer | CrossAttn+EMA | 192 | 80k | 21.97 | 52k | 21.61 | 9h22m | Early Stop |
| **15** | DINOGuidedRestormer | SFT+EMA+强正则 | 192 | 150k | 21.80 | 136k | 21.65 | 27h36m | 完成 |
| 21 | MultiScaleDINOGuidedRestormer | SFT+FPN | 192 | 80k | 21.37 | 2k | 20.29 | 14h05m | Early Stop |
| 31 | DINOEncoderDecoder | 无Restormer | 192 | 80k | 19.90 | 4k | 18.61 | 3h34m | Early Stop |
| 32 | DINOEncoderDecoder+LoRA | LoRA微调 | 192 | 80k | 19.39 | 2k | 18.52 | 2h51m | Early Stop |

> *注: 所有带Early Stop的实验均因验证指标长期未提升而提前终止*

---

## 方案一（1x系列）- DINOGuidedRestormer

### 实验11: 基线实验
- **配置**: gt_size=128, batch=2, 60k iters, MultiStepLR
- **网络**: DINOGuidedRestormer (872M params), dino_gamma=0.4
- **损失**: λ_rec=1.0, λ_ssim=1.0, λ_per=0.1, λ_sem=0.05
- **结果**: 最佳PSNR 22.08 @ 36k iter

### 实验12: 长训练实验
- **配置**: gt_size=192, batch=2, 300k iters, CosineAnnealingRestartCyclicLR
- **网络**: DINOGuidedRestormer (872M params), dino_gamma=0.4
- **损失**: λ_rec=1.0, λ_ssim=0.5, λ_per=0.03, λ_sem=0.01, mixup=True
- **结果**: 最佳PSNR **22.87** @ ~90k iter, Early Stop @ 180k
- **备注**: 过拟合明显，后期PSNR持续下降

### 实验13: EMA稳定训练
- **配置**: gt_size=192, batch=2, 80k iters, warmup=2000, ema_decay=0.999
- **网络**: DINOGuidedRestormer (872M params), dino_gamma=0.35
- **损失**: λ_rec=1.0, λ_ssim=0.8, λ_per=0.05, λ_sem=0.02
- **结果**: 最佳PSNR 21.94 @ 74k iter, 最终21.88
- **备注**: 训练稳定，无明显过拟合

### 实验14: Cross-Attention融合
- **配置**: gt_size=192, batch=2, 80k iters, warmup=2000, ema_decay=0.999
- **网络**: DINOGuidedRestormer (868M params), fusion_type=cross_attention, fusion_num_heads=8
- **损失**: 同实验13
- **结果**: 最佳PSNR 21.97 @ ~52k iter, Early Stop @ 80k
- **备注**: 参数量略少，性能与SFT相近

### 实验15: 抗过拟合优化（长训练+强正则化）
- **配置**: gt_size=192, batch=2, 150k iters, warmup=3000, ema_decay=0.999
- **网络**: DINOGuidedRestormer (872M params), dino_gamma=0.35
- **损失**: λ_rec=1.0, λ_ssim=0.6, λ_per=0.02, λ_sem=0.01
- **正则化**: weight_decay=1e-3, mixup+cutmix
- **学习率**: 三阶段衰减 [50k,50k,50k], restart_weights=[1,0.5,0.1]
- **结果**: 最佳PSNR 21.80 @ 136k iter, 最终21.65
- **训练时长**: 27h36m (1天3小时36分)
- **备注**: 训练稳定但峰值未超越实验12/13，强正则化可能过度抑制了模型容量

---

## 方案二（2x系列）- 多尺度DINO引导

### 实验21: FPN多尺度融合
- **配置**: gt_size=192, batch=2, 80k iters, ema_decay=0.999
- **网络**: MultiScaleDINOGuidedRestormer, dino_extract_layers=[4,8,12], inject_levels=all
- **损失**: 同实验13
- **结果**: 最佳PSNR 21.37 @ 2k iter, 后续下降至~20.2
- **备注**: 多尺度特征未带来提升，训练不稳定

---

## 方案三（3x系列）- 纯DINO Encoder-Decoder

### 实验31: 冻结DINO解码
- **配置**: gt_size=192, batch=2, 80k iters, ema_decay=0.999
- **网络**: DINOEncoderDecoder, extract_layers=[4,8,12], hidden_dims=[384,192,96,48], use_lora=False
- **损失**: 同实验13, semantic_distance=cosine
- **结果**: 最佳PSNR 19.90 @ 4k iter
- **备注**: 无Restormer骨干，性能显著下降

### 实验32: LoRA微调DINO
- **配置**: gt_size=192, batch=2, 80k iters, warmup=3000, layerwise_lr
- **网络**: DINOEncoderDecoder, use_lora=True, lora_r=16, lora_alpha=32
- **优化器**: lr=1e-4, dino_lr_ratio=0.1
- **结果**: 最佳PSNR 19.39 @ 2k iter
- **备注**: LoRA微调反而更差，可能学习率过高

---

## 关键发现

1. **最优配置**: DINOGuidedRestormer + SFT融合 + 192 patch size，PSNR可达22.87
2. **EMA有效**: 使用EMA（decay=0.999）可防止后期性能下降
3. **过拟合风险**: 长训练（300k）容易过拟合，80k迭代配合Early Stopping更稳健
4. **DINO单独不够**: 纯DINO方案（3x系列）性能不足，Restormer骨干网络仍是关键
5. **多尺度无益**: FPN多尺度融合未带来提升，单层DINO特征已足够
6. **正则化过强有害**: 实验15表明过强的正则化（weight_decay=1e-3 + cutmix）反而限制了模型性能

---

## 实验15分析

### 训练曲线特点
- **起步慢**: 2k iter时PSNR仅20.80，4k iter时下降到19.96（warmup阶段不稳定）
- **稳步上升**: 10k-100k iter期间PSNR从21.02缓慢上升到21.74
- **后期平稳**: 100k-150k iter期间PSNR在21.6-21.8之间波动，最佳21.80 @ 136k

### 与实验12/13对比
| 对比项 | 实验12 | 实验13 | 实验15 |
|--------|--------|--------|--------|
| 最佳PSNR | **22.87** | 21.94 | 21.80 |
| 最佳iter | 90k | 74k | 136k |
| 过拟合 | 严重 | 无 | 无 |
| weight_decay | 1e-4 | 5e-4 | 1e-3 |
| 感知损失权重 | 0.03 | 0.05 | 0.02 |

### 失败原因分析
1. **正则化过强**: weight_decay从5e-4提升到1e-3，限制了模型拟合能力
2. **感知损失过低**: λ_per=0.02可能不足以提供足够的高频细节监督
3. **CutMix数据增强**: 可能破坏了低光照图像的整体亮度一致性
4. **三阶段学习率衰减过早**: 50k就开始衰减，可能错过了最佳学习窗口

### 后续建议
1. **实验16**: 保持实验13的正则化强度，仅延长训练到120k + 更激进的后期lr衰减
2. **实验17**: 尝试dino_gamma=0.25，降低DINO特征影响
3. **实验18**: 移除CutMix，仅保留Mixup

---
