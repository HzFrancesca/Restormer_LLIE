# 实验结果汇总

> 最后更新: 2026-01-12

## 实验总结表格

| 实验ID | 架构 | 融合方式 | Patch Size | 迭代次数 | 最佳PSNR | 最佳iter | 最终PSNR | 训练时长 | 状态 |
|--------|------|----------|------------|----------|----------|----------|----------|----------|------|
| 11 | DINOGuidedRestormer | SFT | 128 | 60k | 22.08 | 36k | 21.11 | 3h33m | 完成 |
| 12 | DINOGuidedRestormer | SFT | 192 | 300k | **22.87** | 90k | 20.60 | 19h37m | Early Stop |
| 13 | DINOGuidedRestormer | SFT+EMA | 192 | 80k | 21.94 | 74k | 21.88 | 10h58m | 完成 |
| 14 | DINOGuidedRestormer | CrossAttn+EMA | 192 | 80k | 21.97 | 52k | 21.61 | 9h22m | Early Stop |
| 21 | MultiScaleDINOGuidedRestormer | SFT+FPN | 192 | 80k | 21.37 | 2k | 20.29 | 14h05m | Early Stop |
| 31 | DINOEncoderDecoder | 无Restormer | 192 | 80k | 19.90 | 4k | 18.61 | 3h34m | Early Stop |
| 32 | DINOEncoderDecoder+LoRA | LoRA微调 | 192 | 80k | 19.39 | 2k | 18.52 | 2h51m | Early Stop |

> *注: 所有带Early Stop的实验均因验证指标长期未提升而提前终止*

---

## 方案一（1x系列）- DINOGuidedRestormer

### 实验11: 基线实验
- **配置**: gt_size=128, batch=2, 60k iters, MultiStepLR
- **网络**: DINOGuidedRestormer (872M params), dino_gamma=0.4
- **损失**: λ_rec=1.0, λ_ssim=1.0, λ_per=0.1, λ_sem=0.05
- **结果**: 最佳PSNR 22.08 @ 36k iter

### 实验12: 长训练实验
- **配置**: gt_size=192, batch=2, 300k iters, CosineAnnealingRestartCyclicLR
- **网络**: DINOGuidedRestormer (872M params), dino_gamma=0.4
- **损失**: λ_rec=1.0, λ_ssim=0.5, λ_per=0.03, λ_sem=0.01, mixup=True
- **结果**: 最佳PSNR **22.87** @ ~90k iter, Early Stop @ 180k
- **备注**: 过拟合明显，后期PSNR持续下降

### 实验13: EMA稳定训练
- **配置**: gt_size=192, batch=2, 80k iters, warmup=2000, ema_decay=0.999
- **网络**: DINOGuidedRestormer (872M params), dino_gamma=0.35
- **损失**: λ_rec=1.0, λ_ssim=0.8, λ_per=0.05, λ_sem=0.02
- **结果**: 最佳PSNR 21.94 @ 74k iter, 最终21.88
- **备注**: 训练稳定，无明显过拟合

### 实验14: Cross-Attention融合
- **配置**: gt_size=192, batch=2, 80k iters, warmup=2000, ema_decay=0.999
- **网络**: DINOGuidedRestormer (868M params), fusion_type=cross_attention, fusion_num_heads=8
- **损失**: 同实验13
- **结果**: 最佳PSNR 21.97 @ ~52k iter, Early Stop @ 80k
- **备注**: 参数量略少，性能与SFT相近

---

## 方案二（2x系列）- 多尺度DINO引导

### 实验21: FPN多尺度融合
- **配置**: gt_size=192, batch=2, 80k iters, ema_decay=0.999
- **网络**: MultiScaleDINOGuidedRestormer, dino_extract_layers=[4,8,12], inject_levels=all
- **损失**: 同实验13
- **结果**: 最佳PSNR 21.37 @ 2k iter, 后续下降至~20.2
- **备注**: 多尺度特征未带来提升，训练不稳定

---

## 方案三（3x系列）- 纯DINO Encoder-Decoder

### 实验31: 冻结DINO解码
- **配置**: gt_size=192, batch=2, 80k iters, ema_decay=0.999
- **网络**: DINOEncoderDecoder, extract_layers=[4,8,12], hidden_dims=[384,192,96,48], use_lora=False
- **损失**: 同实验13, semantic_distance=cosine
- **结果**: 最佳PSNR 19.90 @ 4k iter
- **备注**: 无Restormer骨干，性能显著下降

### 实验32: LoRA微调DINO
- **配置**: gt_size=192, batch=2, 80k iters, warmup=3000, layerwise_lr
- **网络**: DINOEncoderDecoder, use_lora=True, lora_r=16, lora_alpha=32
- **优化器**: lr=1e-4, dino_lr_ratio=0.1
- **结果**: 最佳PSNR 19.39 @ 2k iter
- **备注**: LoRA微调反而更差，可能学习率过高

---

## 关键发现

1. **最优配置**: DINOGuidedRestormer + SFT融合 + 192 patch size，PSNR可达22.87
2. **EMA有效**: 使用EMA（decay=0.999）可防止后期性能下降
3. **过拟合风险**: 长训练（300k）容易过拟合，80k迭代配合Early Stopping更稳健
4. **DINO单独不够**: 纯DINO方案（3x系列）性能不足，Restormer骨干网络仍是关键
5. **多尺度无益**: FPN多尺度融合未带来提升，单层DINO特征已足够

---
