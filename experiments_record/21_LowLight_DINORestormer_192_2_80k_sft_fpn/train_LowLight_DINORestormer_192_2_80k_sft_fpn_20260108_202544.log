2026-01-08 20:25:44,071 INFO: 
                ____                _       _____  ____
               / __ ) ____ _ _____ (_)_____/ ___/ / __ \
              / __  |/ __ `// ___// // ___/\__ \ / /_/ /
             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/
            /_____/ \__,_//____//_/ \___//____//_/ |_|
     ______                   __   __                 __      __
    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /
   / / __ / __ \ / __ \ / __  /  / /   / / / // ___// //_/  / /
  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/
  \____/ \____/ \____/ \____/  /_____/\____/ \___//_/|_|  (_)
    
Version Information: 
	BasicSR: 1.2.0+10018c6
	PyTorch: 2.2.0+cu121
	TorchVision: 0.17.0+cu121
2026-01-08 20:25:44,071 INFO: 
  name: LowLight_DINORestormer_192_2_80k_sft_fpn
  model_type: DINOImageRestorationModel
  scale: 1
  num_gpu: 1
  manual_seed: 42
  datasets:[
    train:[
      name: TrainSet
      type: Dataset_PairedImage
      dataroot_gt: ./datasets/LOL-v2/Real_captured/Train/Normal
      dataroot_lq: ./datasets/LOL-v2/Real_captured/Train/Low
      geometric_augs: True
      filename_tmpl: {}
      io_backend:[
        type: disk
      ]
      use_shuffle: True
      num_worker_per_gpu: 4
      batch_size_per_gpu: 2
      mini_batch_sizes: [2]
      iters: [80000]
      gt_size: 192
      gt_sizes: [192]
      dataset_enlarge_ratio: 1
      prefetch_mode: None
      phase: train
      scale: 1
    ]
    val:[
      name: ValSet
      type: Dataset_PairedImage
      dataroot_gt: ./datasets/LOL-v2/Real_captured/Test/Normal
      dataroot_lq: ./datasets/LOL-v2/Real_captured/Test/Low
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 1
    ]
  ]
  network_g:[
    type: MultiScaleDINOGuidedRestormer
    inp_channels: 3
    out_channels: 3
    dim: 48
    num_blocks: [4, 6, 6, 8]
    num_refinement_blocks: 4
    heads: [1, 2, 4, 8]
    ffn_expansion_factor: 2.66
    bias: False
    LayerNorm_type: WithBias
    dino_model: dinov3_vithplus16
    dino_gamma: 0.35
    dino_local_path: E:\2024HZF\Models\facebook\dinov3-vith16plus-pretrain-lvd1689m
    use_dino_guidance: True
    fusion_type: sft
    fusion_num_heads: 4
    dino_extract_layers: [4, 8, 12]
    inject_levels: all
  ]
  path:[
    pretrain_network_g: None
    strict_load_g: False
    resume_state: None
    root: E:\2024HZF\Programs\Restormer_LLIE
    experiments_root: E:\2024HZF\Programs\Restormer_LLIE\experiments\LowLight_DINORestormer_192_2_80k_sft_fpn
    models: E:\2024HZF\Programs\Restormer_LLIE\experiments\LowLight_DINORestormer_192_2_80k_sft_fpn\models
    training_states: E:\2024HZF\Programs\Restormer_LLIE\experiments\LowLight_DINORestormer_192_2_80k_sft_fpn\training_states
    log: E:\2024HZF\Programs\Restormer_LLIE\experiments\LowLight_DINORestormer_192_2_80k_sft_fpn
    visualization: E:\2024HZF\Programs\Restormer_LLIE\experiments\LowLight_DINORestormer_192_2_80k_sft_fpn\visualization
  ]
  train:[
    total_iter: 80000
    warmup_iter: 2000
    use_grad_clip: True
    ema_decay: 0.999
    scheduler:[
      type: CosineAnnealingRestartCyclicLR
      periods: [40000, 40000]
      restart_weights: [1, 0.5]
      eta_mins: [0.0001, 1e-07]
    ]
    mixing_augs:[
      mixup: True
      mixup_beta: 1.0
      use_identity: True
    ]
    optim_g:[
      type: AdamW
      lr: 0.0002
      weight_decay: 0.0005
      betas: [0.9, 0.999]
    ]
    composite_opt:[
      lambda_rec: 1.0
      lambda_ssim: 0.8
      lambda_per: 0.05
      lambda_sem: 0.02
      use_perceptual: True
      use_semantic: True
      dino_gamma: 0.35
    ]
  ]
  val:[
    window_size: 8
    val_freq: 2000.0
    save_img: False
    rgb2bgr: True
    use_image: True
    max_minibatch: 8
    metrics:[
      psnr:[
        type: calculate_psnr
        crop_border: 0
        test_y_channel: False
      ]
    ]
    early_stopping:[
      enabled: True
      patience: 30
      monitor: psnr
    ]
  ]
  logger:[
    print_freq: 500
    save_checkpoint_freq: 10000.0
    use_tb_logger: True
    wandb:[
      project: None
      resume_id: None
    ]
  ]
  dist_params:[
    backend: nccl
    port: 29500
  ]
  is_train: True
  dist: False
  rank: 0
  world_size: 1

2026-01-08 20:25:44,167 INFO: Dataset Dataset_PairedImage - TrainSet is created.
2026-01-08 20:25:44,167 INFO: Training statistics:
	Number of train images: 689
	Dataset enlarge ratio: 1
	Batch size per gpu: 2
	World size (gpu number): 1
	Require iter number per epoch: 345
	Total epochs: 232; iters: 80000.
2026-01-08 20:25:44,173 INFO: Dataset Dataset_PairedImage - ValSet is created.
2026-01-08 20:25:44,173 INFO: Number of val images/folders in ValSet: 100
2026-01-08 20:25:46,538 INFO: Network: MultiScaleDINOGuidedRestormer, with parameters: 874,550,100
2026-01-08 20:25:46,538 INFO: MultiScaleDINOGuidedRestormer(
  (patch_embed): OverlapPatchEmbed(
    (proj): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (encoder_level1): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)
        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)
        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)
        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)
        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (down1_2): Downsample(
    (body): Sequential(
      (0): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelUnshuffle(downscale_factor=2)
    )
  )
  (encoder_level2): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (down2_3): Downsample(
    (body): Sequential(
      (0): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelUnshuffle(downscale_factor=2)
    )
  )
  (encoder_level3): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (down3_4): Downsample(
    (body): Sequential(
      (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelUnshuffle(downscale_factor=2)
    )
  )
  (latent): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (6): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (7): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)
        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (up4_3): Upsample(
    (body): Sequential(
      (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelShuffle(upscale_factor=2)
    )
  )
  (reduce_chan_level3): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (decoder_level3): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)
        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (up3_2): Upsample(
    (body): Sequential(
      (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelShuffle(upscale_factor=2)
    )
  )
  (reduce_chan_level2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (decoder_level2): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (up2_1): Upsample(
    (body): Sequential(
      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): PixelShuffle(upscale_factor=2)
    )
  )
  (decoder_level1): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (refinement): Sequential(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (attn): Attention(
        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (norm2): LayerNorm(
        (body): WithBias_LayerNorm()
      )
      (ffn): FeedForward(
        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)
        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (output): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (dino_extractor): DINOFeatureExtractor(
    (dino): DINOv3ViTModel(
      (embeddings): DINOv3ViTEmbeddings(
        (patch_embeddings): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (rope_embeddings): DINOv3ViTRopePositionEmbedding()
      (layer): ModuleList(
        (0-31): 32 x DINOv3ViTLayer(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attention): DINOv3ViTAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (o_proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (layer_scale1): DINOv3ViTLayerScale()
          (drop_path): Identity()
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (mlp): DINOv3ViTGatedMLP(
            (gate_proj): Linear(in_features=1280, out_features=5120, bias=True)
            (up_proj): Linear(in_features=1280, out_features=5120, bias=True)
            (down_proj): Linear(in_features=5120, out_features=1280, bias=True)
            (act_fn): SiLUActivation()
          )
          (layer_scale2): DINOv3ViTLayerScale()
        )
      )
      (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    )
  )
  (dino_fusion_level2): MultiScaleDINOFusion(
    (fusion): SFTFusion(
      (dino_compress): Sequential(
        (0): Conv2d(1280, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
      )
      (gamma_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (beta_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (dino_fusion_level3): MultiScaleDINOFusion(
    (fusion): SFTFusion(
      (dino_compress): Sequential(
        (0): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
      )
      (gamma_conv): Sequential(
        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (beta_conv): Sequential(
        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (dino_fusion_latent): MultiScaleDINOFusion(
    (fusion): SFTFusion(
      (dino_compress): Sequential(
        (0): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
      )
      (gamma_conv): Sequential(
        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (beta_conv): Sequential(
        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
)
2026-01-08 20:25:46,557 INFO: Use Exponential Moving Average with decay: 0.999
2026-01-08 20:25:48,187 INFO: Using DINO model from network for semantic loss
2026-01-08 20:25:49,113 INFO: Using DINOCompositeLoss: rec=1.0, ssim=0.8, per=0.05, sem=0.02
2026-01-08 20:25:49,114 WARNING: Params dino_extractor.dino.embeddings.cls_token will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.embeddings.mask_token will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.embeddings.register_tokens will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.embeddings.patch_embeddings.weight will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.embeddings.patch_embeddings.bias will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.layer.0.norm1.weight will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.layer.0.norm1.bias will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.layer.0.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.layer.0.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.layer.0.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.layer.0.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,115 WARNING: Params dino_extractor.dino.layer.0.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.norm2.weight will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.norm2.bias will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,116 WARNING: Params dino_extractor.dino.layer.0.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.0.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.norm1.weight will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.norm1.bias will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,117 WARNING: Params dino_extractor.dino.layer.1.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.norm2.weight will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.norm2.bias will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.1.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.2.norm1.weight will not be optimized.
2026-01-08 20:25:49,118 WARNING: Params dino_extractor.dino.layer.2.norm1.bias will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,119 WARNING: Params dino_extractor.dino.layer.2.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.norm2.weight will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.norm2.bias will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,120 WARNING: Params dino_extractor.dino.layer.2.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.norm1.weight will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.norm1.bias will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,121 WARNING: Params dino_extractor.dino.layer.3.norm2.weight will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.norm2.bias will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.3.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,122 WARNING: Params dino_extractor.dino.layer.4.norm1.weight will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.norm1.bias will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,123 WARNING: Params dino_extractor.dino.layer.4.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.norm2.weight will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.norm2.bias will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,124 WARNING: Params dino_extractor.dino.layer.4.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.4.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.4.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.norm1.weight will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.norm1.bias will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,125 WARNING: Params dino_extractor.dino.layer.5.norm2.weight will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.norm2.bias will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.5.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.6.norm1.weight will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.6.norm1.bias will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.6.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.6.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,126 WARNING: Params dino_extractor.dino.layer.6.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.norm2.weight will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.norm2.bias will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,127 WARNING: Params dino_extractor.dino.layer.6.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.6.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.6.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.6.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.norm1.weight will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.norm1.bias will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,128 WARNING: Params dino_extractor.dino.layer.7.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.norm2.weight will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.norm2.bias will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.7.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,129 WARNING: Params dino_extractor.dino.layer.8.norm1.weight will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.norm1.bias will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.norm2.weight will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.norm2.bias will not be optimized.
2026-01-08 20:25:49,130 WARNING: Params dino_extractor.dino.layer.8.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.8.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.8.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.8.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.8.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.8.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.8.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.9.norm1.weight will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.9.norm1.bias will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.9.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.9.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.9.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,131 WARNING: Params dino_extractor.dino.layer.9.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.norm2.weight will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.norm2.bias will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,132 WARNING: Params dino_extractor.dino.layer.9.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.9.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.norm1.weight will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.norm1.bias will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,133 WARNING: Params dino_extractor.dino.layer.10.norm2.weight will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.norm2.bias will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.10.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.11.norm1.weight will not be optimized.
2026-01-08 20:25:49,134 WARNING: Params dino_extractor.dino.layer.11.norm1.bias will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.norm2.weight will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.norm2.bias will not be optimized.
2026-01-08 20:25:49,135 WARNING: Params dino_extractor.dino.layer.11.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.11.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.11.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.11.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.11.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.11.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.11.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.12.norm1.weight will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.12.norm1.bias will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.12.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.12.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.12.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.12.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,136 WARNING: Params dino_extractor.dino.layer.12.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.norm2.weight will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.norm2.bias will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,137 WARNING: Params dino_extractor.dino.layer.12.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.norm1.weight will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.norm1.bias will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,138 WARNING: Params dino_extractor.dino.layer.13.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,139 WARNING: Params dino_extractor.dino.layer.13.norm2.weight will not be optimized.
2026-01-08 20:25:49,139 WARNING: Params dino_extractor.dino.layer.13.norm2.bias will not be optimized.
2026-01-08 20:25:49,139 WARNING: Params dino_extractor.dino.layer.13.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,139 WARNING: Params dino_extractor.dino.layer.13.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,139 WARNING: Params dino_extractor.dino.layer.13.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,139 WARNING: Params dino_extractor.dino.layer.13.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,139 WARNING: Params dino_extractor.dino.layer.13.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.13.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.13.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.norm1.weight will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.norm1.bias will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,140 WARNING: Params dino_extractor.dino.layer.14.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.norm2.weight will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.norm2.bias will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,141 WARNING: Params dino_extractor.dino.layer.14.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.14.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.norm1.weight will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.norm1.bias will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,142 WARNING: Params dino_extractor.dino.layer.15.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.norm2.weight will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.norm2.bias will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,143 WARNING: Params dino_extractor.dino.layer.15.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.norm1.weight will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.norm1.bias will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,144 WARNING: Params dino_extractor.dino.layer.16.norm2.weight will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.norm2.bias will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.16.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.17.norm1.weight will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.17.norm1.bias will not be optimized.
2026-01-08 20:25:49,145 WARNING: Params dino_extractor.dino.layer.17.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.norm2.weight will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.norm2.bias will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,146 WARNING: Params dino_extractor.dino.layer.17.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.17.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.17.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.17.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.norm1.weight will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.norm1.bias will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,147 WARNING: Params dino_extractor.dino.layer.18.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.norm2.weight will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.norm2.bias will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,148 WARNING: Params dino_extractor.dino.layer.18.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.norm1.weight will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.norm1.bias will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,149 WARNING: Params dino_extractor.dino.layer.19.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.norm2.weight will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.norm2.bias will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.19.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.20.norm1.weight will not be optimized.
2026-01-08 20:25:49,150 WARNING: Params dino_extractor.dino.layer.20.norm1.bias will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.norm2.weight will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.norm2.bias will not be optimized.
2026-01-08 20:25:49,151 WARNING: Params dino_extractor.dino.layer.20.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.20.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.20.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.20.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.20.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.20.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.20.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.21.norm1.weight will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.21.norm1.bias will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.21.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.21.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.21.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,152 WARNING: Params dino_extractor.dino.layer.21.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.norm2.weight will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.norm2.bias will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,153 WARNING: Params dino_extractor.dino.layer.21.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.21.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.norm1.weight will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.norm1.bias will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,154 WARNING: Params dino_extractor.dino.layer.22.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.norm2.weight will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.norm2.bias will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,155 WARNING: Params dino_extractor.dino.layer.22.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.22.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.22.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.23.norm1.weight will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.23.norm1.bias will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.23.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.23.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.23.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.23.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,156 WARNING: Params dino_extractor.dino.layer.23.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.norm2.weight will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.norm2.bias will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,157 WARNING: Params dino_extractor.dino.layer.23.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.23.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.23.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.norm1.weight will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.norm1.bias will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,158 WARNING: Params dino_extractor.dino.layer.24.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.norm2.weight will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.norm2.bias will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,159 WARNING: Params dino_extractor.dino.layer.24.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.norm1.weight will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.norm1.bias will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.norm2.weight will not be optimized.
2026-01-08 20:25:49,160 WARNING: Params dino_extractor.dino.layer.25.norm2.bias will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.25.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.25.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.25.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.25.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.25.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.25.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.25.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.26.norm1.weight will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.26.norm1.bias will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.26.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.26.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,161 WARNING: Params dino_extractor.dino.layer.26.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.norm2.weight will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.norm2.bias will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,162 WARNING: Params dino_extractor.dino.layer.26.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.26.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.26.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.26.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.26.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.27.norm1.weight will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.27.norm1.bias will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.27.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.27.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.27.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,163 WARNING: Params dino_extractor.dino.layer.27.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.norm2.weight will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.norm2.bias will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,164 WARNING: Params dino_extractor.dino.layer.27.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.27.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.27.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.norm1.weight will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.norm1.bias will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,165 WARNING: Params dino_extractor.dino.layer.28.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.norm2.weight will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.norm2.bias will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,166 WARNING: Params dino_extractor.dino.layer.28.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.28.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.norm1.weight will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.norm1.bias will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,167 WARNING: Params dino_extractor.dino.layer.29.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,168 WARNING: Params dino_extractor.dino.layer.29.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,168 WARNING: Params dino_extractor.dino.layer.29.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,168 WARNING: Params dino_extractor.dino.layer.29.norm2.weight will not be optimized.
2026-01-08 20:25:49,168 WARNING: Params dino_extractor.dino.layer.29.norm2.bias will not be optimized.
2026-01-08 20:25:49,168 WARNING: Params dino_extractor.dino.layer.29.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,168 WARNING: Params dino_extractor.dino.layer.29.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,168 WARNING: Params dino_extractor.dino.layer.29.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.29.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.29.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.29.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.29.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.30.norm1.weight will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.30.norm1.bias will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.30.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.30.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,169 WARNING: Params dino_extractor.dino.layer.30.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.norm2.weight will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.norm2.bias will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,170 WARNING: Params dino_extractor.dino.layer.30.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.30.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.30.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.30.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.30.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.30.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.31.norm1.weight will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.31.norm1.bias will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.31.attention.k_proj.weight will not be optimized.
2026-01-08 20:25:49,171 WARNING: Params dino_extractor.dino.layer.31.attention.v_proj.weight will not be optimized.
2026-01-08 20:25:49,172 WARNING: Params dino_extractor.dino.layer.31.attention.v_proj.bias will not be optimized.
2026-01-08 20:25:49,172 WARNING: Params dino_extractor.dino.layer.31.attention.q_proj.weight will not be optimized.
2026-01-08 20:25:49,172 WARNING: Params dino_extractor.dino.layer.31.attention.q_proj.bias will not be optimized.
2026-01-08 20:25:49,172 WARNING: Params dino_extractor.dino.layer.31.attention.o_proj.weight will not be optimized.
2026-01-08 20:25:49,172 WARNING: Params dino_extractor.dino.layer.31.attention.o_proj.bias will not be optimized.
2026-01-08 20:25:49,172 WARNING: Params dino_extractor.dino.layer.31.layer_scale1.lambda1 will not be optimized.
2026-01-08 20:25:49,173 WARNING: Params dino_extractor.dino.layer.31.norm2.weight will not be optimized.
2026-01-08 20:25:49,173 WARNING: Params dino_extractor.dino.layer.31.norm2.bias will not be optimized.
2026-01-08 20:25:49,173 WARNING: Params dino_extractor.dino.layer.31.mlp.gate_proj.weight will not be optimized.
2026-01-08 20:25:49,173 WARNING: Params dino_extractor.dino.layer.31.mlp.gate_proj.bias will not be optimized.
2026-01-08 20:25:49,173 WARNING: Params dino_extractor.dino.layer.31.mlp.up_proj.weight will not be optimized.
2026-01-08 20:25:49,173 WARNING: Params dino_extractor.dino.layer.31.mlp.up_proj.bias will not be optimized.
2026-01-08 20:25:49,174 WARNING: Params dino_extractor.dino.layer.31.mlp.down_proj.weight will not be optimized.
2026-01-08 20:25:49,174 WARNING: Params dino_extractor.dino.layer.31.mlp.down_proj.bias will not be optimized.
2026-01-08 20:25:49,174 WARNING: Params dino_extractor.dino.layer.31.layer_scale2.lambda1 will not be optimized.
2026-01-08 20:25:49,174 WARNING: Params dino_extractor.dino.norm.weight will not be optimized.
2026-01-08 20:25:49,174 WARNING: Params dino_extractor.dino.norm.bias will not be optimized.
2026-01-08 20:25:49,175 INFO: Model [DINOImageRestorationModel] is created.
2026-01-08 20:25:57,488 INFO: Start training from epoch: 0, iter: 0
2026-01-08 20:26:06,104 INFO: 
 Updating Patch_Size to 192 and Batch_Size to 2 

2026-01-08 20:29:53,934 INFO: [LowLi..][epoch:  1, iter:     500, lr:(5.000e-05,)] [eta: 10:47:18, time (data): 0.384 (0.000)] l_rec: 8.7790e-02 l_ssim: 1.3438e-01 l_per: 5.3377e+00 l_sem: 2.9496e-01 l_total: 4.6808e-01 
2026-01-08 20:33:39,671 INFO: [LowLi..][epoch:  2, iter:   1,000, lr:(1.000e-04,)] [eta: 10:18:51, time (data): 0.427 (0.001)] l_rec: 6.4073e-02 l_ssim: 3.7557e-01 l_per: 1.1607e+01 l_sem: 5.3616e-01 l_total: 9.5563e-01 
2026-01-08 20:37:34,963 INFO: [LowLi..][epoch:  4, iter:   1,500, lr:(1.500e-04,)] [eta: 10:15:11, time (data): 0.443 (0.000)] l_rec: 1.7921e-01 l_ssim: 2.6885e-01 l_per: 8.3039e+00 l_sem: 6.6925e-01 l_total: 8.2286e-01 
2026-01-08 20:41:22,368 INFO: [LowLi..][epoch:  5, iter:   2,000, lr:(1.994e-04,)] [eta: 10:06:15, time (data): 0.395 (0.001)] l_rec: 1.1736e-01 l_ssim: 2.7759e-01 l_per: 8.3832e+00 l_sem: 2.5618e-01 l_total: 7.6372e-01 
2026-01-08 20:41:54,218 INFO: Validation ValSet,		 # psnr: 21.3708
2026-01-08 20:41:54,219 INFO: Early stopping check: current 'psnr' is 21.3708, best is -inf
2026-01-08 20:41:54,219 INFO: New best metric: 21.3708. Saving best model.
2026-01-08 20:45:54,407 INFO: [LowLi..][epoch:  7, iter:   2,500, lr:(1.990e-04,)] [eta: 10:22:26, time (data): 0.434 (0.001)] l_rec: 8.3106e-02 l_ssim: 1.6259e-01 l_per: 7.0887e+00 l_sem: 3.0385e-01 l_total: 5.7369e-01 
2026-01-08 20:49:41,506 INFO: [LowLi..][epoch:  8, iter:   3,000, lr:(1.986e-04,)] [eta: 10:12:30, time (data): 0.442 (0.000)] l_rec: 7.4451e-02 l_ssim: 2.6383e-01 l_per: 9.0842e+00 l_sem: 4.9507e-01 l_total: 7.4962e-01 
2026-01-08 20:53:36,912 INFO: [LowLi..][epoch: 10, iter:   3,500, lr:(1.981e-04,)] [eta: 10:07:21, time (data): 0.426 (0.000)] l_rec: 1.1314e-01 l_ssim: 2.6628e-01 l_per: 9.8555e+00 l_sem: 7.2130e-01 l_total: 8.3337e-01 
2026-01-08 20:57:24,757 INFO: [LowLi..][epoch: 11, iter:   4,000, lr:(1.976e-04,)] [eta: 10:00:06, time (data): 0.397 (0.000)] l_rec: 9.4765e-02 l_ssim: 1.2211e-01 l_per: 5.3360e+00 l_sem: 3.4731e-01 l_total: 4.6620e-01 
2026-01-09 03:32:11,404 INFO: Validation ValSet,		 # psnr: 20.3744
2026-01-09 03:32:11,404 INFO: Early stopping check: current 'psnr' is 20.3744, best is 21.3708
2026-01-09 03:32:11,404 INFO: Metric did not improve. Patience counter: 1/30
2026-01-09 03:35:40,433 INFO: [LowLi..][epoch: 13, iter:   4,500, lr:(1.969e-04,)] [eta: 5 days, 0:10:18, time (data): 0.381 (0.000)] l_rec: 4.8249e-02 l_ssim: 1.5137e-01 l_per: 7.5051e+00 l_sem: 2.9173e-01 l_total: 5.5043e-01 
2026-01-09 03:39:01,030 INFO: [LowLi..][epoch: 14, iter:   5,000, lr:(1.962e-04,)] [eta: 4 days, 12:16:34, time (data): 0.391 (0.001)] l_rec: 5.9325e-02 l_ssim: 1.3126e-01 l_per: 5.5554e+00 l_sem: 2.6341e-01 l_total: 4.4737e-01 
2026-01-09 03:42:21,608 INFO: [LowLi..][epoch: 15, iter:   5,500, lr:(1.954e-04,)] [eta: 4 days, 2:31:59, time (data): 0.403 (0.001)] l_rec: 1.1269e-01 l_ssim: 2.1734e-01 l_per: 6.5745e+00 l_sem: 2.9055e-01 l_total: 6.2110e-01 
2026-01-09 03:45:50,662 INFO: [LowLi..][epoch: 17, iter:   6,000, lr:(1.946e-04,)] [eta: 3 days, 18:25:59, time (data): 0.384 (0.001)] l_rec: 1.0901e-01 l_ssim: 2.5484e-01 l_per: 7.0938e+00 l_sem: 4.7352e-01 l_total: 6.7704e-01 
2026-01-09 03:46:22,681 INFO: Validation ValSet,		 # psnr: 20.0598
2026-01-09 03:46:22,682 INFO: Early stopping check: current 'psnr' is 20.0598, best is 21.3708
2026-01-09 03:46:22,682 INFO: Metric did not improve. Patience counter: 2/30
2026-01-09 03:49:43,463 INFO: [LowLi..][epoch: 18, iter:   6,500, lr:(1.936e-04,)] [eta: 3 days, 11:38:41, time (data): 0.389 (0.001)] l_rec: 1.0555e-01 l_ssim: 1.5511e-01 l_per: 6.3012e+00 l_sem: 1.9515e-01 l_total: 5.4860e-01 
2026-01-09 03:53:12,460 INFO: [LowLi..][epoch: 20, iter:   7,000, lr:(1.926e-04,)] [eta: 3 days, 5:44:53, time (data): 0.374 (0.000)] l_rec: 7.3750e-02 l_ssim: 8.8174e-02 l_per: 4.4878e+00 l_sem: 2.1492e-01 l_total: 3.7298e-01 
2026-01-09 03:56:32,997 INFO: [LowLi..][epoch: 21, iter:   7,500, lr:(1.916e-04,)] [eta: 3 days, 0:36:25, time (data): 0.379 (0.000)] l_rec: 1.1978e-01 l_ssim: 1.4262e-01 l_per: 4.4233e+00 l_sem: 2.7755e-01 l_total: 4.6059e-01 
2026-01-09 04:00:01,883 INFO: [LowLi..][epoch: 23, iter:   8,000, lr:(1.905e-04,)] [eta: 2 days, 20:07:20, time (data): 0.397 (0.001)] l_rec: 8.5590e-02 l_ssim: 2.9021e-01 l_per: 1.1641e+01 l_sem: 7.4117e-01 l_total: 9.1464e-01 
2026-01-09 04:00:34,647 INFO: Validation ValSet,		 # psnr: 20.1942
2026-01-09 04:00:34,647 INFO: Early stopping check: current 'psnr' is 20.1942, best is 21.3708
2026-01-09 04:00:34,647 INFO: Metric did not improve. Patience counter: 3/30
2026-01-09 04:03:55,384 INFO: [LowLi..][epoch: 24, iter:   8,500, lr:(1.893e-04,)] [eta: 2 days, 16:12:57, time (data): 0.399 (0.000)] l_rec: 1.7320e-01 l_ssim: 1.8313e-01 l_per: 6.2335e+00 l_sem: 2.9665e-01 l_total: 6.3732e-01 
2026-01-09 04:07:24,334 INFO: [LowLi..][epoch: 26, iter:   9,000, lr:(1.880e-04,)] [eta: 2 days, 12:40:56, time (data): 0.379 (0.000)] l_rec: 8.3163e-02 l_ssim: 9.5426e-02 l_per: 5.0241e+00 l_sem: 1.1646e-01 l_total: 4.1304e-01 
2026-01-09 04:10:44,844 INFO: [LowLi..][epoch: 27, iter:   9,500, lr:(1.867e-04,)] [eta: 2 days, 9:29:50, time (data): 0.383 (0.000)] l_rec: 1.0121e-01 l_ssim: 1.8225e-01 l_per: 6.3395e+00 l_sem: 2.4308e-01 l_total: 5.6885e-01 
2026-01-09 04:14:13,723 INFO: [LowLi..][epoch: 29, iter:  10,000, lr:(1.854e-04,)] [eta: 2 days, 6:38:29, time (data): 0.390 (0.000)] l_rec: 6.2529e-02 l_ssim: 1.0219e-01 l_per: 4.6048e+00 l_sem: 2.9755e-01 l_total: 3.8047e-01 
2026-01-09 04:14:13,724 INFO: Saving models and training states.
2026-01-09 04:14:54,091 INFO: Validation ValSet,		 # psnr: 20.4705
2026-01-09 04:14:54,091 INFO: Early stopping check: current 'psnr' is 20.4705, best is 21.3708
2026-01-09 04:14:54,092 INFO: Metric did not improve. Patience counter: 4/30
2026-01-09 04:18:14,661 INFO: [LowLi..][epoch: 30, iter:  10,500, lr:(1.839e-04,)] [eta: 2 days, 4:06:39, time (data): 0.370 (0.000)] l_rec: 1.1398e-01 l_ssim: 3.0119e-01 l_per: 9.5596e+00 l_sem: 7.3054e-01 l_total: 8.4751e-01 
2026-01-09 04:21:35,201 INFO: [LowLi..][epoch: 31, iter:  11,000, lr:(1.825e-04,)] [eta: 2 days, 1:44:02, time (data): 0.376 (0.000)] l_rec: 4.3514e-02 l_ssim: 1.3512e-01 l_per: 6.1393e+00 l_sem: 4.6396e-01 l_total: 4.6786e-01 
2026-01-09 04:25:04,161 INFO: [LowLi..][epoch: 33, iter:  11,500, lr:(1.810e-04,)] [eta: 1 day, 23:34:22, time (data): 0.386 (0.001)] l_rec: 1.2019e-01 l_ssim: 1.5615e-01 l_per: 6.7640e+00 l_sem: 1.9635e-01 l_total: 5.8724e-01 
2026-01-09 04:28:24,840 INFO: [LowLi..][epoch: 34, iter:  12,000, lr:(1.794e-04,)] [eta: 1 day, 21:34:26, time (data): 0.394 (0.000)] l_rec: 5.8053e-02 l_ssim: 1.8031e-01 l_per: 8.3751e+00 l_sem: 6.5663e-01 l_total: 6.3419e-01 
2026-01-09 04:28:56,646 INFO: Validation ValSet,		 # psnr: 20.3294
2026-01-09 04:28:56,647 INFO: Early stopping check: current 'psnr' is 20.3294, best is 21.3708
2026-01-09 04:28:56,647 INFO: Metric did not improve. Patience counter: 5/30
2026-01-09 04:32:26,005 INFO: [LowLi..][epoch: 36, iter:  12,500, lr:(1.778e-04,)] [eta: 1 day, 19:47:27, time (data): 0.396 (0.000)] l_rec: 7.9170e-02 l_ssim: 2.4716e-01 l_per: 8.6952e+00 l_sem: 5.0105e-01 l_total: 7.2168e-01 
2026-01-09 04:35:46,744 INFO: [LowLi..][epoch: 37, iter:  13,000, lr:(1.761e-04,)] [eta: 1 day, 18:04:56, time (data): 0.366 (0.000)] l_rec: 9.0177e-02 l_ssim: 1.7647e-01 l_per: 6.7252e+00 l_sem: 5.5927e-01 l_total: 5.7880e-01 
2026-01-09 04:39:15,833 INFO: [LowLi..][epoch: 39, iter:  13,500, lr:(1.744e-04,)] [eta: 1 day, 16:30:27, time (data): 0.392 (0.000)] l_rec: 8.7815e-02 l_ssim: 1.0865e-01 l_per: 4.9166e+00 l_sem: 4.7123e-01 l_total: 4.2999e-01 
2026-01-09 04:42:36,474 INFO: [LowLi..][epoch: 40, iter:  14,000, lr:(1.727e-04,)] [eta: 1 day, 15:01:47, time (data): 0.387 (0.000)] l_rec: 6.7803e-02 l_ssim: 1.5214e-01 l_per: 6.9732e+00 l_sem: 3.3645e-01 l_total: 5.4491e-01 
2026-01-09 04:43:08,323 INFO: Validation ValSet,		 # psnr: 20.3556
2026-01-09 04:43:08,324 INFO: Early stopping check: current 'psnr' is 20.3556, best is 21.3708
2026-01-09 04:43:08,324 INFO: Metric did not improve. Patience counter: 6/30
2026-01-09 04:46:37,353 INFO: [LowLi..][epoch: 42, iter:  14,500, lr:(1.709e-04,)] [eta: 1 day, 13:42:03, time (data): 0.384 (0.000)] l_rec: 1.8057e-01 l_ssim: 2.6618e-01 l_per: 6.4497e+00 l_sem: 2.2626e-01 l_total: 7.2053e-01 
2026-01-09 04:49:58,059 INFO: [LowLi..][epoch: 43, iter:  15,000, lr:(1.691e-04,)] [eta: 1 day, 12:24:27, time (data): 0.388 (0.001)] l_rec: 7.5811e-02 l_ssim: 1.7834e-01 l_per: 7.9153e+00 l_sem: 5.2189e-01 l_total: 6.2468e-01 
2026-01-09 04:53:26,949 INFO: [LowLi..][epoch: 45, iter:  15,500, lr:(1.673e-04,)] [eta: 1 day, 11:12:13, time (data): 0.369 (0.000)] l_rec: 8.6288e-02 l_ssim: 7.2915e-02 l_per: 3.7146e+00 l_sem: 3.7618e-01 l_total: 3.3787e-01 
2026-01-09 04:56:47,663 INFO: [LowLi..][epoch: 46, iter:  16,000, lr:(1.655e-04,)] [eta: 1 day, 10:03:44, time (data): 0.382 (0.001)] l_rec: 6.3760e-02 l_ssim: 1.3641e-01 l_per: 5.7769e+00 l_sem: 3.6054e-01 l_total: 4.6894e-01 
2026-01-09 04:57:19,613 INFO: Validation ValSet,		 # psnr: 20.3791
2026-01-09 04:57:19,614 INFO: Early stopping check: current 'psnr' is 20.3791, best is 21.3708
2026-01-09 04:57:19,614 INFO: Metric did not improve. Patience counter: 7/30
2026-01-09 05:00:40,322 INFO: [LowLi..][epoch: 47, iter:  16,500, lr:(1.636e-04,)] [eta: 1 day, 9:01:15, time (data): 0.389 (0.000)] l_rec: 5.6932e-02 l_ssim: 9.4375e-02 l_per: 4.6577e+00 l_sem: 8.4430e-02 l_total: 3.6701e-01 
2026-01-09 05:04:09,373 INFO: [LowLi..][epoch: 49, iter:  17,000, lr:(1.617e-04,)] [eta: 1 day, 8:00:45, time (data): 0.384 (0.001)] l_rec: 1.0599e-01 l_ssim: 9.9547e-02 l_per: 4.2168e+00 l_sem: 2.8497e-01 l_total: 4.0217e-01 
2026-01-09 05:07:30,015 INFO: [LowLi..][epoch: 50, iter:  17,500, lr:(1.598e-04,)] [eta: 1 day, 7:03:00, time (data): 0.396 (0.000)] l_rec: 1.1462e-01 l_ssim: 1.8344e-01 l_per: 6.1978e+00 l_sem: 2.5812e-01 l_total: 5.7642e-01 
2026-01-09 05:10:58,847 INFO: [LowLi..][epoch: 52, iter:  18,000, lr:(1.578e-04,)] [eta: 1 day, 6:08:45, time (data): 0.374 (0.000)] l_rec: 1.2442e-01 l_ssim: 1.7073e-01 l_per: 6.1092e+00 l_sem: 2.4386e-01 l_total: 5.7134e-01 
2026-01-09 05:11:31,040 INFO: Validation ValSet,		 # psnr: 20.1515
2026-01-09 05:11:31,041 INFO: Early stopping check: current 'psnr' is 20.1515, best is 21.3708
2026-01-09 05:11:31,041 INFO: Metric did not improve. Patience counter: 8/30
2026-01-09 05:14:51,797 INFO: [LowLi..][epoch: 53, iter:  18,500, lr:(1.559e-04,)] [eta: 1 day, 5:18:35, time (data): 0.384 (0.000)] l_rec: 8.2400e-02 l_ssim: 1.6276e-01 l_per: 5.6602e+00 l_sem: 1.0365e-01 l_total: 4.9769e-01 
2026-01-09 05:18:20,804 INFO: [LowLi..][epoch: 55, iter:  19,000, lr:(1.539e-04,)] [eta: 1 day, 4:29:34, time (data): 0.385 (0.001)] l_rec: 5.9026e-02 l_ssim: 7.4743e-02 l_per: 3.8713e+00 l_sem: 4.1154e-01 l_total: 3.2062e-01 
2026-01-09 05:21:41,489 INFO: [LowLi..][epoch: 56, iter:  19,500, lr:(1.520e-04,)] [eta: 1 day, 3:42:27, time (data): 0.383 (0.000)] l_rec: 1.2770e-01 l_ssim: 1.8619e-01 l_per: 6.2232e+00 l_sem: 3.9357e-01 l_total: 5.9568e-01 
2026-01-09 05:25:10,618 INFO: [LowLi..][epoch: 58, iter:  20,000, lr:(1.500e-04,)] [eta: 1 day, 2:57:57, time (data): 0.380 (0.000)] l_rec: 4.7009e-02 l_ssim: 1.1231e-01 l_per: 6.1738e+00 l_sem: 3.3113e-01 l_total: 4.5217e-01 
2026-01-09 05:25:10,619 INFO: Saving models and training states.
2026-01-09 05:25:52,169 INFO: Validation ValSet,		 # psnr: 20.3179
2026-01-09 05:25:52,169 INFO: Early stopping check: current 'psnr' is 20.3179, best is 21.3708
2026-01-09 05:25:52,169 INFO: Metric did not improve. Patience counter: 9/30
2026-01-09 05:29:13,037 INFO: [LowLi..][epoch: 59, iter:  20,500, lr:(1.480e-04,)] [eta: 1 day, 2:17:04, time (data): 0.387 (0.000)] l_rec: 6.2761e-02 l_ssim: 3.2709e-01 l_per: 1.0517e+01 l_sem: 5.3820e-01 l_total: 8.6105e-01 
2026-01-09 05:32:41,860 INFO: [LowLi..][epoch: 61, iter:  21,000, lr:(1.461e-04,)] [eta: 1 day, 1:36:22, time (data): 0.382 (0.000)] l_rec: 5.9492e-02 l_ssim: 1.4023e-01 l_per: 4.1860e+00 l_sem: 3.1295e-01 l_total: 3.8723e-01 
2026-01-09 05:36:02,533 INFO: [LowLi..][epoch: 62, iter:  21,500, lr:(1.441e-04,)] [eta: 1 day, 0:57:01, time (data): 0.370 (0.000)] l_rec: 1.2432e-01 l_ssim: 1.0914e-01 l_per: 4.5092e+00 l_sem: 2.2280e-01 l_total: 4.4155e-01 
2026-01-09 05:39:23,159 INFO: [LowLi..][epoch: 63, iter:  22,000, lr:(1.422e-04,)] [eta: 1 day, 0:19:18, time (data): 0.394 (0.000)] l_rec: 4.2191e-02 l_ssim: 1.5437e-01 l_per: 7.4423e+00 l_sem: 1.9865e-01 l_total: 5.4178e-01 
2026-01-09 05:39:55,739 INFO: Validation ValSet,		 # psnr: 20.0873
2026-01-09 05:39:55,739 INFO: Early stopping check: current 'psnr' is 20.0873, best is 21.3708
2026-01-09 05:39:55,739 INFO: Metric did not improve. Patience counter: 10/30
2026-01-09 05:43:24,837 INFO: [LowLi..][epoch: 65, iter:  22,500, lr:(1.402e-04,)] [eta: 23:44:52, time (data): 0.389 (0.000)] l_rec: 3.5199e-02 l_ssim: 1.4980e-01 l_per: 6.7476e+00 l_sem: 4.9314e-01 l_total: 5.0228e-01 
2026-01-09 05:46:45,372 INFO: [LowLi..][epoch: 66, iter:  23,000, lr:(1.383e-04,)] [eta: 23:10:03, time (data): 0.381 (0.000)] l_rec: 6.7154e-02 l_ssim: 8.9404e-02 l_per: 4.9797e+00 l_sem: 2.1621e-01 l_total: 3.9199e-01 
2026-01-09 05:50:14,302 INFO: [LowLi..][epoch: 68, iter:  23,500, lr:(1.364e-04,)] [eta: 22:36:55, time (data): 0.375 (0.001)] l_rec: 7.4191e-02 l_ssim: 1.2039e-01 l_per: 5.3216e+00 l_sem: 3.1118e-01 l_total: 4.4281e-01 
2026-01-09 05:53:34,920 INFO: [LowLi..][epoch: 69, iter:  24,000, lr:(1.346e-04,)] [eta: 22:04:42, time (data): 0.375 (0.000)] l_rec: 1.3613e-01 l_ssim: 3.5447e-01 l_per: 9.7810e+00 l_sem: 5.4648e-01 l_total: 9.1968e-01 
2026-01-09 05:54:05,989 INFO: Validation ValSet,		 # psnr: 20.0851
2026-01-09 05:54:05,990 INFO: Early stopping check: current 'psnr' is 20.0851, best is 21.3708
2026-01-09 05:54:05,990 INFO: Metric did not improve. Patience counter: 11/30
2026-01-09 05:57:34,944 INFO: [LowLi..][epoch: 71, iter:  24,500, lr:(1.327e-04,)] [eta: 21:35:08, time (data): 0.388 (0.000)] l_rec: 7.5552e-02 l_ssim: 1.7230e-01 l_per: 6.6480e+00 l_sem: 2.2598e-01 l_total: 5.5031e-01 
2026-01-09 06:00:55,511 INFO: [LowLi..][epoch: 72, iter:  25,000, lr:(1.309e-04,)] [eta: 21:05:09, time (data): 0.393 (0.001)] l_rec: 4.0191e-02 l_ssim: 1.7917e-01 l_per: 3.8676e+00 l_sem: 2.3679e-01 l_total: 3.8164e-01 
2026-01-09 06:04:24,470 INFO: [LowLi..][epoch: 74, iter:  25,500, lr:(1.291e-04,)] [eta: 20:36:31, time (data): 0.388 (0.000)] l_rec: 6.0082e-02 l_ssim: 1.1329e-01 l_per: 5.3217e+00 l_sem: 2.4326e-01 l_total: 4.2166e-01 
2026-01-09 06:07:45,088 INFO: [LowLi..][epoch: 75, iter:  26,000, lr:(1.273e-04,)] [eta: 20:08:33, time (data): 0.391 (0.001)] l_rec: 6.5475e-02 l_ssim: 1.6845e-01 l_per: 4.8773e+00 l_sem: 1.6840e-01 l_total: 4.4747e-01 
2026-01-09 06:08:16,148 INFO: Validation ValSet,		 # psnr: 20.1934
2026-01-09 06:08:16,148 INFO: Early stopping check: current 'psnr' is 20.1934, best is 21.3708
2026-01-09 06:08:16,148 INFO: Metric did not improve. Patience counter: 12/30
2026-01-09 06:11:44,934 INFO: [LowLi..][epoch: 77, iter:  26,500, lr:(1.256e-04,)] [eta: 19:42:50, time (data): 0.377 (0.000)] l_rec: 6.9286e-02 l_ssim: 2.1014e-01 l_per: 6.5688e+00 l_sem: 3.4307e-01 l_total: 5.7270e-01 
2026-01-09 06:15:05,209 INFO: [LowLi..][epoch: 78, iter:  27,000, lr:(1.239e-04,)] [eta: 19:16:38, time (data): 0.380 (0.000)] l_rec: 6.4740e-02 l_ssim: 1.0261e-01 l_per: 4.9709e+00 l_sem: 3.3751e-01 l_total: 4.0212e-01 
2026-01-09 06:18:25,541 INFO: [LowLi..][epoch: 79, iter:  27,500, lr:(1.222e-04,)] [eta: 18:51:16, time (data): 0.388 (0.000)] l_rec: 3.3583e-02 l_ssim: 5.9621e-02 l_per: 3.9148e+00 l_sem: 2.8006e-01 l_total: 2.8262e-01 
2026-01-09 06:21:54,211 INFO: [LowLi..][epoch: 81, iter:  28,000, lr:(1.206e-04,)] [eta: 18:26:57, time (data): 0.384 (0.001)] l_rec: 4.2826e-02 l_ssim: 1.1101e-01 l_per: 5.0322e+00 l_sem: 2.4469e-01 l_total: 3.8813e-01 
2026-01-09 06:29:35,241 INFO: Validation ValSet,		 # psnr: 20.1775
2026-01-09 06:29:35,241 INFO: Early stopping check: current 'psnr' is 20.1775, best is 21.3708
2026-01-09 06:29:35,241 INFO: Metric did not improve. Patience counter: 13/30
2026-01-09 06:32:55,707 INFO: [LowLi..][epoch: 82, iter:  28,500, lr:(1.190e-04,)] [eta: 18:16:59, time (data): 0.386 (0.000)] l_rec: 3.5768e-02 l_ssim: 1.1998e-01 l_per: 5.5572e+00 l_sem: 4.7878e-01 l_total: 4.1919e-01 
2026-01-09 06:36:24,625 INFO: [LowLi..][epoch: 84, iter:  29,000, lr:(1.175e-04,)] [eta: 17:53:44, time (data): 0.384 (0.000)] l_rec: 5.0594e-02 l_ssim: 2.0143e-01 l_per: 4.6534e+00 l_sem: 2.6346e-01 l_total: 4.4968e-01 
2026-01-09 06:39:45,231 INFO: [LowLi..][epoch: 85, iter:  29,500, lr:(1.161e-04,)] [eta: 17:30:54, time (data): 0.382 (0.001)] l_rec: 8.9627e-02 l_ssim: 2.6268e-01 l_per: 9.1877e+00 l_sem: 6.5698e-01 l_total: 7.7229e-01 
2026-01-09 06:43:13,975 INFO: [LowLi..][epoch: 87, iter:  30,000, lr:(1.146e-04,)] [eta: 17:08:58, time (data): 0.388 (0.001)] l_rec: 6.9734e-02 l_ssim: 2.6179e-01 l_per: 5.0140e+00 l_sem: 1.9879e-01 l_total: 5.3384e-01 
2026-01-09 06:43:13,976 INFO: Saving models and training states.
2026-01-09 06:43:52,887 INFO: Validation ValSet,		 # psnr: 20.1890
2026-01-09 06:43:52,887 INFO: Early stopping check: current 'psnr' is 20.1890, best is 21.3708
2026-01-09 06:43:52,887 INFO: Metric did not improve. Patience counter: 14/30
2026-01-09 06:47:13,569 INFO: [LowLi..][epoch: 88, iter:  30,500, lr:(1.133e-04,)] [eta: 16:48:27, time (data): 0.379 (0.000)] l_rec: 7.5963e-02 l_ssim: 1.2900e-01 l_per: 4.8077e+00 l_sem: 4.5000e-01 l_total: 4.2854e-01 
2026-01-09 06:50:42,451 INFO: [LowLi..][epoch: 90, iter:  31,000, lr:(1.120e-04,)] [eta: 16:27:40, time (data): 0.431 (0.000)] l_rec: 8.8504e-02 l_ssim: 2.0403e-01 l_per: 8.3977e+00 l_sem: 6.1821e-01 l_total: 6.8397e-01 
2026-01-09 06:54:02,923 INFO: [LowLi..][epoch: 91, iter:  31,500, lr:(1.107e-04,)] [eta: 16:07:13, time (data): 0.378 (0.001)] l_rec: 3.1977e-02 l_ssim: 9.8054e-02 l_per: 4.8796e+00 l_sem: 2.3226e-01 l_total: 3.5904e-01 
2026-01-09 06:57:31,907 INFO: [LowLi..][epoch: 93, iter:  32,000, lr:(1.096e-04,)] [eta: 15:47:31, time (data): 0.389 (0.001)] l_rec: 3.5354e-02 l_ssim: 1.7026e-01 l_per: 6.0755e+00 l_sem: 4.1039e-01 l_total: 4.8354e-01 
2026-01-09 06:58:03,177 INFO: Validation ValSet,		 # psnr: 20.0775
2026-01-09 06:58:03,177 INFO: Early stopping check: current 'psnr' is 20.0775, best is 21.3708
2026-01-09 06:58:03,178 INFO: Metric did not improve. Patience counter: 15/30
2026-01-09 07:01:23,894 INFO: [LowLi..][epoch: 94, iter:  32,500, lr:(1.084e-04,)] [eta: 15:28:52, time (data): 0.388 (0.000)] l_rec: 6.5338e-02 l_ssim: 2.0922e-01 l_per: 8.1338e+00 l_sem: 3.1266e-01 l_total: 6.4566e-01 
2026-01-09 07:04:44,558 INFO: [LowLi..][epoch: 95, iter:  33,000, lr:(1.074e-04,)] [eta: 15:09:56, time (data): 0.388 (0.000)] l_rec: 2.0122e-02 l_ssim: 7.8433e-02 l_per: 4.6067e+00 l_sem: 3.2449e-01 l_total: 3.1969e-01 
2026-01-09 07:08:13,638 INFO: [LowLi..][epoch: 97, iter:  33,500, lr:(1.064e-04,)] [eta: 14:51:39, time (data): 0.372 (0.000)] l_rec: 6.4169e-02 l_ssim: 1.9245e-01 l_per: 8.7611e+00 l_sem: 3.2893e-01 l_total: 6.6276e-01 
2026-01-09 07:11:34,275 INFO: [LowLi..][epoch: 98, iter:  34,000, lr:(1.055e-04,)] [eta: 14:33:37, time (data): 0.397 (0.000)] l_rec: 1.0628e-01 l_ssim: 2.2587e-01 l_per: 5.9837e+00 l_sem: 1.8052e-01 l_total: 5.8978e-01 
2026-01-09 07:12:05,603 INFO: Validation ValSet,		 # psnr: 20.0833
2026-01-09 07:12:05,604 INFO: Early stopping check: current 'psnr' is 20.0833, best is 21.3708
2026-01-09 07:12:05,604 INFO: Metric did not improve. Patience counter: 16/30
2026-01-09 07:15:34,545 INFO: [LowLi..][epoch:100, iter:  34,500, lr:(1.046e-04,)] [eta: 14:16:52, time (data): 0.390 (0.000)] l_rec: 4.6291e-02 l_ssim: 1.2773e-01 l_per: 5.4948e+00 l_sem: 2.5476e-01 l_total: 4.2831e-01 
2026-01-09 07:18:54,985 INFO: [LowLi..][epoch:101, iter:  35,000, lr:(1.038e-04,)] [eta: 13:59:39, time (data): 0.389 (0.001)] l_rec: 4.9871e-02 l_ssim: 1.1772e-01 l_per: 5.6881e+00 l_sem: 3.3934e-01 l_total: 4.3524e-01 
2026-01-09 07:22:23,892 INFO: [LowLi..][epoch:103, iter:  35,500, lr:(1.031e-04,)] [eta: 13:42:59, time (data): 0.370 (0.001)] l_rec: 7.8502e-02 l_ssim: 1.0279e-01 l_per: 4.7783e+00 l_sem: 2.2460e-01 l_total: 4.0414e-01 
2026-01-09 07:25:44,411 INFO: [LowLi..][epoch:104, iter:  36,000, lr:(1.024e-04,)] [eta: 13:26:31, time (data): 0.401 (0.000)] l_rec: 5.0117e-02 l_ssim: 2.3287e-01 l_per: 7.2279e+00 l_sem: 3.9819e-01 l_total: 6.0577e-01 
2026-01-09 07:26:16,380 INFO: Validation ValSet,		 # psnr: 20.0939
2026-01-09 07:26:16,380 INFO: Early stopping check: current 'psnr' is 20.0939, best is 21.3708
2026-01-09 07:26:16,380 INFO: Metric did not improve. Patience counter: 17/30
2026-01-09 07:29:45,515 INFO: [LowLi..][epoch:106, iter:  36,500, lr:(1.019e-04,)] [eta: 13:11:13, time (data): 0.391 (0.001)] l_rec: 1.1028e-01 l_ssim: 1.6659e-01 l_per: 5.8208e+00 l_sem: 2.3186e-01 l_total: 5.3923e-01 
2026-01-09 07:33:05,810 INFO: [LowLi..][epoch:107, iter:  37,000, lr:(1.014e-04,)] [eta: 12:55:26, time (data): 0.373 (0.001)] l_rec: 8.5645e-02 l_ssim: 1.4103e-01 l_per: 5.3976e+00 l_sem: 2.0905e-01 l_total: 4.7253e-01 
2026-01-09 07:36:34,523 INFO: [LowLi..][epoch:109, iter:  37,500, lr:(1.010e-04,)] [eta: 12:40:09, time (data): 0.380 (0.001)] l_rec: 6.5188e-02 l_ssim: 6.8696e-02 l_per: 3.6783e+00 l_sem: 3.5884e-01 l_total: 3.1124e-01 
2026-01-09 07:39:54,944 INFO: [LowLi..][epoch:110, iter:  38,000, lr:(1.006e-04,)] [eta: 12:25:00, time (data): 0.393 (0.000)] l_rec: 1.0135e-01 l_ssim: 1.2445e-01 l_per: 5.4485e+00 l_sem: 4.2265e-01 l_total: 4.8178e-01 
2026-01-09 07:40:26,373 INFO: Validation ValSet,		 # psnr: 19.7700
2026-01-09 07:40:26,373 INFO: Early stopping check: current 'psnr' is 19.7700, best is 21.3708
2026-01-09 07:40:26,374 INFO: Metric did not improve. Patience counter: 18/30
2026-01-09 07:43:46,889 INFO: [LowLi..][epoch:111, iter:  38,500, lr:(1.003e-04,)] [eta: 12:10:45, time (data): 0.386 (0.001)] l_rec: 8.3151e-02 l_ssim: 1.5932e-01 l_per: 6.2173e+00 l_sem: 3.4319e-01 l_total: 5.2834e-01 
2026-01-09 07:47:15,738 INFO: [LowLi..][epoch:113, iter:  39,000, lr:(1.002e-04,)] [eta: 11:56:21, time (data): 0.370 (0.001)] l_rec: 5.6464e-02 l_ssim: 1.5153e-01 l_per: 5.9716e+00 l_sem: 2.7173e-01 l_total: 4.8170e-01 
2026-01-09 07:50:36,239 INFO: [LowLi..][epoch:114, iter:  39,500, lr:(1.000e-04,)] [eta: 11:42:05, time (data): 0.382 (0.000)] l_rec: 1.0781e-01 l_ssim: 2.2090e-01 l_per: 5.5638e+00 l_sem: 2.4786e-01 l_total: 5.6768e-01 
2026-01-09 07:54:05,252 INFO: [LowLi..][epoch:116, iter:  40,000, lr:(1.000e-04,)] [eta: 11:28:14, time (data): 0.377 (0.000)] l_rec: 8.3974e-02 l_ssim: 2.9171e-01 l_per: 4.5889e+00 l_sem: 4.4315e-01 l_total: 5.5565e-01 
2026-01-09 07:54:05,252 INFO: Saving models and training states.
2026-01-09 07:54:44,815 INFO: Validation ValSet,		 # psnr: 19.8052
2026-01-09 07:54:44,815 INFO: Early stopping check: current 'psnr' is 19.8052, best is 21.3708
2026-01-09 07:54:44,815 INFO: Metric did not improve. Patience counter: 19/30
2026-01-09 07:58:05,411 INFO: [LowLi..][epoch:117, iter:  40,500, lr:(1.000e-04,)] [eta: 11:15:08, time (data): 0.383 (0.001)] l_rec: 2.6187e-02 l_ssim: 7.2113e-02 l_per: 4.1867e+00 l_sem: 2.8520e-01 l_total: 2.9892e-01 
2026-01-09 08:01:34,202 INFO: [LowLi..][epoch:119, iter:  41,000, lr:(9.990e-05,)] [eta: 11:01:46, time (data): 0.374 (0.000)] l_rec: 5.8613e-02 l_ssim: 1.7491e-01 l_per: 4.0086e+00 l_sem: 2.1816e-01 l_total: 4.0333e-01 
2026-01-09 08:04:54,676 INFO: [LowLi..][epoch:120, iter:  41,500, lr:(9.970e-05,)] [eta: 10:48:31, time (data): 0.369 (0.000)] l_rec: 6.0071e-02 l_ssim: 8.0846e-02 l_per: 3.8821e+00 l_sem: 1.5578e-01 l_total: 3.2197e-01 
2026-01-09 08:08:23,492 INFO: [LowLi..][epoch:122, iter:  42,000, lr:(9.944e-05,)] [eta: 10:35:37, time (data): 0.386 (0.000)] l_rec: 9.2845e-02 l_ssim: 5.0979e-02 l_per: 2.5381e+00 l_sem: 3.9572e-01 l_total: 2.6845e-01 
2026-01-09 08:09:09,553 INFO: Validation ValSet,		 # psnr: 20.1361
2026-01-09 08:09:09,553 INFO: Early stopping check: current 'psnr' is 20.1361, best is 21.3708
2026-01-09 08:09:09,554 INFO: Metric did not improve. Patience counter: 20/30
2026-01-09 08:12:30,078 INFO: [LowLi..][epoch:123, iter:  42,500, lr:(9.909e-05,)] [eta: 10:23:30, time (data): 0.380 (0.001)] l_rec: 8.9446e-02 l_ssim: 1.4843e-01 l_per: 6.9463e+00 l_sem: 2.4475e-01 l_total: 5.6040e-01 
2026-01-09 08:15:50,569 INFO: [LowLi..][epoch:124, iter:  43,000, lr:(9.867e-05,)] [eta: 10:10:55, time (data): 0.387 (0.000)] l_rec: 2.1456e-02 l_ssim: 5.8209e-02 l_per: 3.7802e+00 l_sem: 3.0931e-01 l_total: 2.6322e-01 
2026-01-09 08:19:19,486 INFO: [LowLi..][epoch:126, iter:  43,500, lr:(9.817e-05,)] [eta: 9:58:39, time (data): 0.372 (0.000)] l_rec: 7.6481e-02 l_ssim: 1.5398e-01 l_per: 6.7654e+00 l_sem: 2.5777e-01 l_total: 5.4310e-01 
2026-01-09 08:22:40,015 INFO: [LowLi..][epoch:127, iter:  44,000, lr:(9.761e-05,)] [eta: 9:46:28, time (data): 0.383 (0.000)] l_rec: 3.1685e-02 l_ssim: 9.2965e-02 l_per: 5.6821e+00 l_sem: 2.8863e-01 l_total: 3.9594e-01 
2026-01-09 08:23:11,551 INFO: Validation ValSet,		 # psnr: 19.7808
2026-01-09 08:23:11,552 INFO: Early stopping check: current 'psnr' is 19.7808, best is 21.3708
2026-01-09 08:23:11,552 INFO: Metric did not improve. Patience counter: 21/30
2026-01-09 08:26:40,531 INFO: [LowLi..][epoch:129, iter:  44,500, lr:(9.696e-05,)] [eta: 9:35:02, time (data): 0.379 (0.000)] l_rec: 2.8981e-02 l_ssim: 8.1538e-02 l_per: 3.8388e+00 l_sem: 2.5583e-01 l_total: 2.9127e-01 
2026-01-09 08:30:01,064 INFO: [LowLi..][epoch:130, iter:  45,000, lr:(9.625e-05,)] [eta: 9:23:14, time (data): 0.380 (0.000)] l_rec: 3.8867e-02 l_ssim: 5.7519e-02 l_per: 4.0852e+00 l_sem: 3.0216e-01 l_total: 2.9519e-01 
2026-01-09 08:33:30,048 INFO: [LowLi..][epoch:132, iter:  45,500, lr:(9.546e-05,)] [eta: 9:11:43, time (data): 0.366 (0.000)] l_rec: 7.6044e-02 l_ssim: 1.0618e-01 l_per: 4.2303e+00 l_sem: 1.7175e-01 l_total: 3.7594e-01 
2026-01-09 08:36:50,466 INFO: [LowLi..][epoch:133, iter:  46,000, lr:(9.460e-05,)] [eta: 9:00:17, time (data): 0.383 (0.000)] l_rec: 2.3547e-02 l_ssim: 6.3482e-02 l_per: 4.1442e+00 l_sem: 2.7008e-01 l_total: 2.8695e-01 
2026-01-09 08:37:21,979 INFO: Validation ValSet,		 # psnr: 20.0311
2026-01-09 08:37:21,980 INFO: Early stopping check: current 'psnr' is 20.0311, best is 21.3708
2026-01-09 08:37:21,980 INFO: Metric did not improve. Patience counter: 22/30
2026-01-09 08:40:50,855 INFO: [LowLi..][epoch:135, iter:  46,500, lr:(9.368e-05,)] [eta: 8:49:30, time (data): 0.388 (0.000)] l_rec: 7.7676e-02 l_ssim: 9.9046e-02 l_per: 4.2468e+00 l_sem: 4.8900e-01 l_total: 3.7903e-01 
2026-01-09 08:44:11,321 INFO: [LowLi..][epoch:136, iter:  47,000, lr:(9.269e-05,)] [eta: 8:38:24, time (data): 0.380 (0.001)] l_rec: 1.0096e-01 l_ssim: 2.5354e-01 l_per: 7.3309e+00 l_sem: 4.0729e-01 l_total: 6.7848e-01 
2026-01-09 08:47:40,232 INFO: [LowLi..][epoch:138, iter:  47,500, lr:(9.163e-05,)] [eta: 8:27:33, time (data): 0.365 (0.000)] l_rec: 1.0080e-01 l_ssim: 1.1252e-01 l_per: 4.9497e+00 l_sem: 1.5750e-01 l_total: 4.4145e-01 
2026-01-09 08:51:00,665 INFO: [LowLi..][epoch:139, iter:  48,000, lr:(9.051e-05,)] [eta: 8:16:46, time (data): 0.383 (0.000)] l_rec: 5.5439e-02 l_ssim: 1.2365e-01 l_per: 6.2124e+00 l_sem: 4.1753e-01 l_total: 4.7333e-01 
2026-01-09 08:51:53,836 INFO: Validation ValSet,		 # psnr: 20.0655
2026-01-09 08:51:53,837 INFO: Early stopping check: current 'psnr' is 20.0655, best is 21.3708
2026-01-09 08:51:53,837 INFO: Metric did not improve. Patience counter: 23/30
2026-01-09 08:55:14,409 INFO: [LowLi..][epoch:140, iter:  48,500, lr:(8.932e-05,)] [eta: 8:06:42, time (data): 0.381 (0.000)] l_rec: 3.3497e-02 l_ssim: 7.0547e-02 l_per: 3.8806e+00 l_sem: 3.6178e-01 l_total: 2.9120e-01 
2026-01-09 08:58:43,345 INFO: [LowLi..][epoch:142, iter:  49,000, lr:(8.808e-05,)] [eta: 7:56:18, time (data): 0.389 (0.000)] l_rec: 6.8110e-02 l_ssim: 1.9436e-01 l_per: 4.6465e+00 l_sem: 2.0993e-01 l_total: 4.6012e-01 
2026-01-09 09:02:04,062 INFO: [LowLi..][epoch:143, iter:  49,500, lr:(8.678e-05,)] [eta: 7:45:56, time (data): 0.376 (0.000)] l_rec: 4.0693e-02 l_ssim: 9.6981e-02 l_per: 5.4809e+00 l_sem: 4.2806e-01 l_total: 4.0088e-01 
2026-01-09 09:05:32,962 INFO: [LowLi..][epoch:145, iter:  50,000, lr:(8.542e-05,)] [eta: 7:35:48, time (data): 0.386 (0.000)] l_rec: 1.1946e-01 l_ssim: 1.8849e-01 l_per: 5.5444e+00 l_sem: 1.7485e-01 l_total: 5.5098e-01 
2026-01-09 09:05:32,962 INFO: Saving models and training states.
2026-01-09 09:06:13,628 INFO: Validation ValSet,		 # psnr: 20.0240
2026-01-09 09:06:13,628 INFO: Early stopping check: current 'psnr' is 20.0240, best is 21.3708
2026-01-09 09:06:13,629 INFO: Metric did not improve. Patience counter: 24/30
2026-01-09 09:09:34,521 INFO: [LowLi..][epoch:146, iter:  50,500, lr:(8.400e-05,)] [eta: 7:26:07, time (data): 0.387 (0.000)] l_rec: 4.5470e-02 l_ssim: 9.9678e-02 l_per: 4.8894e+00 l_sem: 1.7574e-01 l_total: 3.7320e-01 
2026-01-09 09:13:03,309 INFO: [LowLi..][epoch:148, iter:  51,000, lr:(8.253e-05,)] [eta: 7:16:14, time (data): 0.389 (0.000)] l_rec: 5.8726e-02 l_ssim: 2.0625e-01 l_per: 6.3165e+00 l_sem: 4.4737e-01 l_total: 5.4850e-01 
2026-01-09 09:16:23,877 INFO: [LowLi..][epoch:149, iter:  51,500, lr:(8.102e-05,)] [eta: 7:06:24, time (data): 0.389 (0.000)] l_rec: 5.5659e-02 l_ssim: 1.6957e-01 l_per: 7.5242e+00 l_sem: 4.4738e-01 l_total: 5.7648e-01 
2026-01-09 09:19:52,781 INFO: [LowLi..][epoch:151, iter:  52,000, lr:(7.945e-05,)] [eta: 6:56:46, time (data): 0.370 (0.000)] l_rec: 4.1106e-02 l_ssim: 1.0352e-01 l_per: 6.1613e+00 l_sem: 2.6899e-01 l_total: 4.3737e-01 
2026-01-09 09:20:24,265 INFO: Validation ValSet,		 # psnr: 19.9625
2026-01-09 09:20:24,265 INFO: Early stopping check: current 'psnr' is 19.9625, best is 21.3708
2026-01-09 09:20:24,265 INFO: Metric did not improve. Patience counter: 25/30
2026-01-09 09:23:44,995 INFO: [LowLi..][epoch:152, iter:  52,500, lr:(7.784e-05,)] [eta: 6:47:27, time (data): 0.371 (0.000)] l_rec: 5.4941e-02 l_ssim: 7.3544e-02 l_per: 3.5143e+00 l_sem: 1.3438e-01 l_total: 2.9218e-01 
2026-01-09 09:27:13,772 INFO: [LowLi..][epoch:154, iter:  53,000, lr:(7.619e-05,)] [eta: 6:38:03, time (data): 0.384 (0.000)] l_rec: 7.7763e-02 l_ssim: 6.4475e-02 l_per: 3.6445e+00 l_sem: 3.6661e-01 l_total: 3.1890e-01 
2026-01-09 09:30:34,267 INFO: [LowLi..][epoch:155, iter:  53,500, lr:(7.450e-05,)] [eta: 6:28:41, time (data): 0.391 (0.000)] l_rec: 5.1931e-02 l_ssim: 1.1789e-01 l_per: 5.7503e+00 l_sem: 3.5784e-01 l_total: 4.4092e-01 
2026-01-09 09:33:54,889 INFO: [LowLi..][epoch:156, iter:  54,000, lr:(7.277e-05,)] [eta: 6:19:25, time (data): 0.393 (0.000)] l_rec: 4.3315e-02 l_ssim: 1.0750e-01 l_per: 5.6054e+00 l_sem: 1.7342e-01 l_total: 4.1305e-01 
2026-01-09 09:34:26,820 INFO: Validation ValSet,		 # psnr: 20.0824
2026-01-09 09:34:26,821 INFO: Early stopping check: current 'psnr' is 20.0824, best is 21.3708
2026-01-09 09:34:26,821 INFO: Metric did not improve. Patience counter: 26/30
2026-01-09 09:37:55,975 INFO: [LowLi..][epoch:158, iter:  54,500, lr:(7.100e-05,)] [eta: 6:10:36, time (data): 0.382 (0.001)] l_rec: 6.4827e-02 l_ssim: 1.2733e-01 l_per: 6.5072e+00 l_sem: 3.2961e-01 l_total: 4.9865e-01 
2026-01-09 09:41:16,619 INFO: [LowLi..][epoch:159, iter:  55,000, lr:(6.920e-05,)] [eta: 6:01:33, time (data): 0.394 (0.000)] l_rec: 2.0916e-02 l_ssim: 8.4399e-02 l_per: 3.8999e+00 l_sem: 3.5274e-01 l_total: 2.9048e-01 
2026-01-09 09:44:45,653 INFO: [LowLi..][epoch:161, iter:  55,500, lr:(6.738e-05,)] [eta: 5:52:39, time (data): 0.388 (0.000)] l_rec: 5.0914e-02 l_ssim: 8.6729e-02 l_per: 4.9136e+00 l_sem: 4.1427e-01 l_total: 3.7426e-01 
2026-01-09 09:48:06,322 INFO: [LowLi..][epoch:162, iter:  56,000, lr:(6.552e-05,)] [eta: 5:43:48, time (data): 0.397 (0.001)] l_rec: 1.0121e-01 l_ssim: 1.2808e-01 l_per: 5.8717e+00 l_sem: 2.3711e-01 l_total: 5.0201e-01 
2026-01-09 09:48:37,802 INFO: Validation ValSet,		 # psnr: 20.1183
2026-01-09 09:48:37,802 INFO: Early stopping check: current 'psnr' is 20.1183, best is 21.3708
2026-01-09 09:48:37,802 INFO: Metric did not improve. Patience counter: 27/30
2026-01-09 09:52:06,803 INFO: [LowLi..][epoch:164, iter:  56,500, lr:(6.364e-05,)] [eta: 5:35:20, time (data): 0.378 (0.000)] l_rec: 2.1354e-02 l_ssim: 3.7009e-02 l_per: 3.1239e+00 l_sem: 2.8784e-01 l_total: 2.1291e-01 
2026-01-09 09:55:27,331 INFO: [LowLi..][epoch:165, iter:  57,000, lr:(6.175e-05,)] [eta: 5:26:40, time (data): 0.380 (0.001)] l_rec: 8.5700e-02 l_ssim: 7.1280e-02 l_per: 3.9334e+00 l_sem: 1.4699e-01 l_total: 3.4234e-01 
2026-01-09 09:58:56,274 INFO: [LowLi..][epoch:167, iter:  57,500, lr:(5.983e-05,)] [eta: 5:18:09, time (data): 0.385 (0.000)] l_rec: 6.3338e-02 l_ssim: 1.8617e-01 l_per: 7.0222e+00 l_sem: 3.7032e-01 l_total: 5.7079e-01 
2026-01-09 10:02:16,899 INFO: [LowLi..][epoch:168, iter:  58,000, lr:(5.790e-05,)] [eta: 5:09:40, time (data): 0.382 (0.001)] l_rec: 4.1924e-02 l_ssim: 1.4717e-01 l_per: 5.3375e+00 l_sem: 3.8709e-01 l_total: 4.3427e-01 
2026-01-09 10:02:48,172 INFO: Validation ValSet,		 # psnr: 20.5011
2026-01-09 10:02:48,172 INFO: Early stopping check: current 'psnr' is 20.5011, best is 21.3708
2026-01-09 10:02:48,173 INFO: Metric did not improve. Patience counter: 28/30
2026-01-09 10:06:17,341 INFO: [LowLi..][epoch:170, iter:  58,500, lr:(5.595e-05,)] [eta: 5:01:31, time (data): 0.380 (0.000)] l_rec: 5.5441e-02 l_ssim: 1.0234e-01 l_per: 5.5984e+00 l_sem: 4.1137e-01 l_total: 4.2546e-01 
2026-01-09 10:09:38,176 INFO: [LowLi..][epoch:171, iter:  59,000, lr:(5.400e-05,)] [eta: 4:53:12, time (data): 0.392 (0.000)] l_rec: 3.6914e-02 l_ssim: 1.3820e-01 l_per: 6.5652e+00 l_sem: 5.3034e-01 l_total: 4.8634e-01 
2026-01-09 10:12:58,726 INFO: [LowLi..][epoch:172, iter:  59,500, lr:(5.204e-05,)] [eta: 4:44:58, time (data): 0.380 (0.000)] l_rec: 7.0749e-02 l_ssim: 1.9006e-01 l_per: 3.9635e+00 l_sem: 2.4968e-01 l_total: 4.2596e-01 
2026-01-09 10:16:27,820 INFO: [LowLi..][epoch:174, iter:  60,000, lr:(5.008e-05,)] [eta: 4:36:51, time (data): 0.384 (0.000)] l_rec: 5.1642e-02 l_ssim: 1.1928e-01 l_per: 5.3359e+00 l_sem: 2.1019e-01 l_total: 4.1806e-01 
2026-01-09 10:16:27,820 INFO: Saving models and training states.
2026-01-09 10:17:07,400 INFO: Validation ValSet,		 # psnr: 20.4441
2026-01-09 10:17:07,401 INFO: Early stopping check: current 'psnr' is 20.4441, best is 21.3708
2026-01-09 10:17:07,401 INFO: Metric did not improve. Patience counter: 29/30
2026-01-09 10:20:28,130 INFO: [LowLi..][epoch:175, iter:  60,500, lr:(4.812e-05,)] [eta: 4:29:00, time (data): 0.396 (0.001)] l_rec: 4.5867e-02 l_ssim: 2.2968e-01 l_per: 5.4320e+00 l_sem: 1.3370e-01 l_total: 5.0389e-01 
2026-01-09 10:23:57,102 INFO: [LowLi..][epoch:177, iter:  61,000, lr:(4.616e-05,)] [eta: 4:21:02, time (data): 0.376 (0.000)] l_rec: 3.8102e-02 l_ssim: 1.3434e-01 l_per: 5.2514e+00 l_sem: 3.2764e-01 l_total: 4.1469e-01 
2026-01-09 10:27:17,835 INFO: [LowLi..][epoch:178, iter:  61,500, lr:(4.420e-05,)] [eta: 4:13:06, time (data): 0.383 (0.000)] l_rec: 5.6841e-02 l_ssim: 1.0312e-01 l_per: 4.1341e+00 l_sem: 2.1891e-01 l_total: 3.5042e-01 
2026-01-09 10:30:47,032 INFO: [LowLi..][epoch:180, iter:  62,000, lr:(4.226e-05,)] [eta: 4:05:17, time (data): 0.389 (0.001)] l_rec: 1.1341e-01 l_ssim: 2.8380e-01 l_per: 8.4630e+00 l_sem: 5.3740e-01 l_total: 7.7435e-01 
2026-01-09 10:31:18,296 INFO: Validation ValSet,		 # psnr: 20.2922
2026-01-09 10:31:18,297 INFO: Early stopping check: current 'psnr' is 20.2922, best is 21.3708
2026-01-09 10:31:18,297 INFO: Metric did not improve. Patience counter: 30/30
2026-01-09 10:31:18,297 INFO: Early stopping triggered after 30 validations without improvement.
2026-01-09 10:31:18,297 INFO: Early stopping triggered. Terminating training.
2026-01-09 10:31:18,298 INFO: End of training. Time consumed: 14:05:20
2026-01-09 10:31:18,298 INFO: Save the latest model.
2026-01-09 10:31:56,805 INFO: Validation ValSet,		 # psnr: 20.2922
2026-01-09 10:31:56,805 INFO: Early stopping check: current 'psnr' is 20.2922, best is 21.3708
2026-01-09 10:31:56,806 INFO: Metric did not improve. Patience counter: 31/30
2026-01-09 10:31:56,806 INFO: Early stopping triggered after 31 validations without improvement.
